{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#TF-Estimator-MNIST-Example\" data-toc-modified-id=\"TF-Estimator-MNIST-Example-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>TF Estimator MNIST Example</a></span></li><li><span><a href=\"#TF-Slim-MNIST-Example\" data-toc-modified-id=\"TF-Slim-MNIST-Example-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>TF Slim MNIST Example</a></span></li><li><span><a href=\"#TFLearn-MNIST-Example\" data-toc-modified-id=\"TFLearn-MNIST-Example-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>TFLearn MNIST Example</a></span></li><li><span><a href=\"#Pretty-Tensor-MNIST-Example\" data-toc-modified-id=\"Pretty-Tensor-MNIST-Example-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Pretty Tensor MNIST Example</a></span></li><li><span><a href=\"#Sonnet-MNIST-Example\" data-toc-modified-id=\"Sonnet-MNIST-Example-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Sonnet MNIST Example</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Level Libraries for TensorFlow <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Estimator MNIST Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmprvcqgu07\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_steps': None, '_task_type': 'worker', '_save_checkpoints_secs': 600, '_service': None, '_task_id': 0, '_master': '', '_session_config': None, '_num_worker_replicas': 1, '_keep_checkpoint_max': 5, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff9d15f5fd0>, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_is_chief': True, '_save_summary_steps': 100, '_model_dir': '/tmp/tmprvcqgu07', '_num_ps_replicas': 0, '_tf_random_seed': None}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmprvcqgu07/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.4365, step = 1\n",
      "INFO:tensorflow:global_step/sec: 597.996\n",
      "INFO:tensorflow:loss = 1.47152, step = 101 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 553.29\n",
      "INFO:tensorflow:loss = 0.728581, step = 201 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 519.498\n",
      "INFO:tensorflow:loss = 0.89795, step = 301 (0.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 503.414\n",
      "INFO:tensorflow:loss = 0.743328, step = 401 (0.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 539.251\n",
      "INFO:tensorflow:loss = 0.413222, step = 501 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 572.327\n",
      "INFO:tensorflow:loss = 0.416304, step = 601 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 543.99\n",
      "INFO:tensorflow:loss = 0.459793, step = 701 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 687.748\n",
      "INFO:tensorflow:loss = 0.501756, step = 801 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 654.217\n",
      "INFO:tensorflow:loss = 0.666772, step = 901 (0.153 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmprvcqgu07/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.426257.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-15-02:27:45\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmprvcqgu07/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-15-02:27:45\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.8856, global_step = 1000, loss = 0.40996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.88559997, 'global_step': 1000, 'loss': 0.40995964}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = input_data.read_data_sets(os.path.join('.', 'mnist'),\n",
    "                                  one_hot=False\n",
    "                                  )\n",
    "x_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "x_test = mnist.test.images\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "n_classes = 10\n",
    "batch_size = 100\n",
    "n_steps = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode):\n",
    "    \"\"\" define the model function\n",
    "    \"\"\"\n",
    "    espec_op = tf.estimator.EstimatorSpec\n",
    "    # features is a dict as per Estimator specifications\n",
    "    x = features['images']\n",
    "    # define the network\n",
    "    layer_1 = tf.layers.dense(x, 32)\n",
    "    layer_2 = tf.layers.dense(layer_1, 32)\n",
    "    logits = tf.layers.dense(layer_2, n_classes)\n",
    "\n",
    "    # define predicted classes\n",
    "    predicted_classes = tf.argmax(logits, axis=1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        espec = espec_op(mode,\n",
    "                         predictions=predicted_classes\n",
    "                         )\n",
    "    else:\n",
    "        # define loss and optimizer\n",
    "        entropy_op = tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "        loss_op = tf.reduce_mean(entropy_op(logits=logits,\n",
    "                                            labels=tf.cast(labels,\n",
    "                                                           dtype=tf.int32)\n",
    "                                            )\n",
    "                                 )\n",
    "        optimizer = tf.train.GradientDescentOptimizer(\n",
    "            learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss_op, global_step=tf.train.get_global_step())\n",
    "\n",
    "        # define accuracy\n",
    "        accuracy_op = tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predicted_classes)\n",
    "\n",
    "        espec = espec_op(mode=mode,\n",
    "                         predictions=predicted_classes,\n",
    "                         loss=loss_op,\n",
    "                         train_op=train_op,\n",
    "                         eval_metric_ops={'accuracy': accuracy_op}\n",
    "                         )\n",
    "\n",
    "    return espec\n",
    "\n",
    "\n",
    "# create estimator object\n",
    "model = tf.estimator.Estimator(model_fn)\n",
    "\n",
    "# train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': x_train},\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "model.train(train_input_fn, steps=n_steps)\n",
    "\n",
    "# evaluate the model\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': x_test},\n",
    "    y=y_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)\n",
    "model.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Slim MNIST Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/training/python/training/training.py:412: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n",
      "INFO:tensorflow:Starting Session.\n",
      "INFO:tensorflow:Saving checkpoint to path ./slim_logs/model.ckpt\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:Starting Queues.\n",
      "INFO:tensorflow:global step 100: loss = 2.2669 (0.010 sec/step)\n",
      "INFO:tensorflow:global step 200: loss = 2.2025 (0.010 sec/step)\n",
      "INFO:tensorflow:global step 300: loss = 2.1257 (0.010 sec/step)\n",
      "INFO:tensorflow:global step 400: loss = 2.0419 (0.009 sec/step)\n",
      "INFO:tensorflow:global step 500: loss = 1.9532 (0.009 sec/step)\n",
      "INFO:tensorflow:global step 600: loss = 1.8733 (0.010 sec/step)\n",
      "INFO:tensorflow:global step 700: loss = 1.8002 (0.010 sec/step)\n",
      "INFO:tensorflow:global step 800: loss = 1.7273 (0.010 sec/step)\n",
      "INFO:tensorflow:global step 900: loss = 1.6688 (0.010 sec/step)\n",
      "INFO:tensorflow:global step 1000: loss = 1.6132 (0.010 sec/step)\n",
      "INFO:tensorflow:Stopping Training.\n",
      "INFO:tensorflow:Finished training! Saving model to disk.\n",
      "final loss=1.6131552457809448\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib import slim\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_classes = 10  \n",
    "n_steps = 1000\n",
    "\n",
    "# let us get the data\n",
    "mnist = input_data.read_data_sets(os.path.join('.', 'mnist'), one_hot=True)\n",
    "\n",
    "X_train = mnist.train.images\n",
    "X_train = tf.convert_to_tensor(X_train)\n",
    "Y_train = mnist.train.labels\n",
    "Y_train = tf.convert_to_tensor(Y_train)\n",
    "\n",
    "\n",
    "def mlp(x):\n",
    "    net = slim.fully_connected(x, 32, scope='fc1')\n",
    "    net = slim.dropout(net, 0.5, scope='dropout1')\n",
    "    net = slim.fully_connected(net, 32, scope='fc2')\n",
    "    net = slim.dropout(net, 0.5, scope='dropout2')\n",
    "    net = slim.fully_connected(net, n_classes, activation_fn=None, scope='fc3')\n",
    "    return net\n",
    "\n",
    "\n",
    "# Define the model\n",
    "logits = mlp(X_train)\n",
    "\n",
    "# Define the loss functions and get the total loss\n",
    "loss = tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y_train)\n",
    "total_loss = tf.losses.get_total_loss()\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "\n",
    "# Run the training\n",
    "final_loss = slim.learning.train(\n",
    "    train_op,\n",
    "    logdir='./slim_logs',\n",
    "    number_of_steps=n_steps,\n",
    "    log_every_n_steps=100)\n",
    "\n",
    "print('final loss={}'.format(final_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFLearn MNIST Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import tflearn\n",
    "import tflearn.datasets.mnist as mnist\n",
    "import os\n",
    "\n",
    "batch_size = 100\n",
    "n_classes = 10\n",
    "n_epochs = 10\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = mnist.load_data(\n",
    "    data_dir=os.path.join('.', 'mnist'), one_hot=True)\n",
    "\n",
    "# Build deep neural network\n",
    "input_layer = tflearn.input_data(shape=[None, 784])\n",
    "layer1 = tflearn.fully_connected(input_layer,\n",
    "                                 10,\n",
    "                                 activation='relu'\n",
    "                                 )\n",
    "layer2 = tflearn.fully_connected(layer1,\n",
    "                                 10,\n",
    "                                 activation='relu'\n",
    "                                 )\n",
    "output = tflearn.fully_connected(layer2,\n",
    "                                 n_classes,\n",
    "                                 activation='softmax'\n",
    "                                 )\n",
    "\n",
    "net = tflearn.regression(output,\n",
    "                         optimizer='adam',\n",
    "                         metric=tflearn.metrics.Accuracy(),\n",
    "                         loss='categorical_crossentropy'\n",
    "                         )\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    n_epoch=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    show_metric=True,\n",
    "    run_id='dense_model')\n",
    "\n",
    "score = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretty Tensor MNIST Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "import prettytensor as pt\n",
    "from prettytensor.tutorial import data_utils\n",
    "import os\n",
    "data_utils.WORK_DIRECTORY = os.path.join('.', 'mnist')\n",
    "\n",
    "# get the data\n",
    "X_train, Y_train = data_utils.mnist(training=True)\n",
    "X_test, Y_test = data_utils.mnist(training=False)\n",
    "\n",
    "# define hyperparameters\n",
    "\n",
    "batch_size = 100     # number of samples that would\n",
    "# be used to learn the parameters in a batch\n",
    "n_classes = 10       # number of outputs, i.e. digits 0 to 9\n",
    "n_epochs = 10        # number of ietrations for learning the parameters\n",
    "n_batches = int(X_train.shape[0] / batch_size)\n",
    "n_samples_in_train_batch = 60000 // batch_size\n",
    "n_samples_in_test_batch = 10000 // batch_size\n",
    "\n",
    "# define inputs and outputs\n",
    "\n",
    "X = tf.placeholder(tf.float32, [batch_size, 28, 28, 1])\n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10])\n",
    "\n",
    "# define the model\n",
    "\n",
    "X = pt.wrap(X)\n",
    "\n",
    "model = (X.\n",
    "         flatten().\n",
    "         fully_connected(10).\n",
    "         softmax_classifier(n_classes, labels=Y)\n",
    "         )\n",
    "\n",
    "# define evaluator (metrics), optimizer and training functions\n",
    "\n",
    "evaluator = model.softmax.evaluate_classifier(Y)\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "trainer = pt.apply_optimizer(optimizer, losses=[model.loss])\n",
    "\n",
    "runner = pt.train.Runner()\n",
    "\n",
    "with tf.Session() as tfs:\n",
    "    for epoch in range(0, n_epochs):\n",
    "        # shuffle the training data\n",
    "        X_train, Y_train = data_utils.permute_data((X_train, Y_train))\n",
    "\n",
    "        runner.train_model(\n",
    "            trainer,\n",
    "            model.loss,\n",
    "            n_samples_in_train_batch,\n",
    "            feed_vars=(X, Y),\n",
    "            feed_data=pt.train.feed_numpy(batch_size, X_train, Y_train),\n",
    "            print_every=600\n",
    "        )\n",
    "\n",
    "        score = runner.evaluate_model(\n",
    "            evaluator,\n",
    "            n_samples_in_test_batch,\n",
    "            feed_vars=(X, Y),\n",
    "            feed_data=pt.train.feed_numpy(batch_size, X_test, Y_test)\n",
    "        )\n",
    "\n",
    "        print('Accuracy after {} epochs {} \\n'.\n",
    "              format(epoch + 1, score[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sonnet MNIST Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import os\n",
    "import sonnet as snt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "\n",
    "class MNIST(snt.AbstractModule):\n",
    "\n",
    "    def __init__(self, mnist_part, batch_size, name='MNIST'):\n",
    "\n",
    "        super(MNIST, self).__init__(name=name)\n",
    "\n",
    "        self._X = tf.constant(mnist_part.images, dtype=tf.float32)\n",
    "        self._Y = tf.constant(mnist_part.labels, dtype=tf.float32)\n",
    "        self._batch_size = batch_size\n",
    "        self._M = mnist_part.num_examples\n",
    "\n",
    "    def _build(self):\n",
    "        idx = tf.random_uniform([self._batch_size], 0, self._M, tf.int64)\n",
    "        X = tf.gather(self._X, idx)\n",
    "        Y = tf.gather(self._Y, idx)\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "class MLP(snt.AbstractModule):\n",
    "    def __init__(self, output_sizes, name='mlp'):\n",
    "        super(MLP, self).__init__(name=name)\n",
    "\n",
    "        self._layers = []\n",
    "\n",
    "        for output_size in output_sizes:\n",
    "            self._layers.append(snt.Linear(output_size=output_size))\n",
    "\n",
    "    def _build(self, X):\n",
    "\n",
    "        # add the input layer\n",
    "        model = tf.sigmoid(self._layers[0](X))\n",
    "\n",
    "        # add hidden layers\n",
    "        for i in range(1, len(self._layers) - 1):\n",
    "            model = tf.sigmoid(self._layers[i](model))\n",
    "\n",
    "        # add output layer\n",
    "        model = tf.nn.softmax(self._layers[len(self._layers) - 1](model))\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "n_classes = 10\n",
    "n_epochs = 10\n",
    "\n",
    "mnist = input_data.read_data_sets(os.path.join('.', 'mnist'),\n",
    "                                  one_hot=True\n",
    "                                  )\n",
    "train = MNIST(mnist.train, batch_size=batch_size)\n",
    "test = MNIST(mnist.test, batch_size=batch_size)\n",
    "\n",
    "X_train, Y_train = train()\n",
    "X_test, Y_test = test()\n",
    "\n",
    "model = MLP([20, n_classes])\n",
    "\n",
    "Y_train_hat = model(X_train)\n",
    "Y_test_hat = model(X_test)\n",
    "\n",
    "\n",
    "def loss(Y_hat, Y):\n",
    "    return -tf.reduce_sum(Y * tf.log(Y_hat))\n",
    "\n",
    "\n",
    "L_train = loss(Y_train_hat, Y_train)\n",
    "L_test = loss(Y_test_hat, Y_test)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=0.01).minimize(L_train)\n",
    "\n",
    "with tf.Session() as tfs:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_val, _ = tfs.run((L_train, optimizer))\n",
    "        print('Epoch : {} Training Loss : {}'.format(epoch, loss_val))\n",
    "\n",
    "    loss_val = tfs.run(L_test)\n",
    "    print('Test loss : {}'.format(loss_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
