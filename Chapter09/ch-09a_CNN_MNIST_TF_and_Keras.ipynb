{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#CNN-with-TensorFlow-for-MNIST-Data\" data-toc-modified-id=\"CNN-with-TensorFlow-for-MNIST-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>CNN with TensorFlow for MNIST Data</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for MNIST with TensorFlow and Keras <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:1.13.1\n",
      "TensorFlow:1.4.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "print(\"NumPy:{}\".format(np.__version__))\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(123)\n",
    "print(\"TensorFlow:{}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETSLIB_HOME = '../datasetslib'\n",
    "import sys\n",
    "if not DATASETSLIB_HOME in sys.path:\n",
    "    sys.path.append(DATASETSLIB_HOME)\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import datasetslib\n",
    "\n",
    "datasetslib.datasets_root = os.path.join(os.path.expanduser('~'),'datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/armando/datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /home/armando/datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/armando/datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/armando/datasets/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(os.path.join(datasetslib.datasets_root,'mnist'), one_hot=True)\n",
    "\n",
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "Y_train = mnist.train.labels\n",
    "Y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with TensorFlow for MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_classes = 10  # 0-9 digits\n",
    "n_width = 28\n",
    "n_height = 28\n",
    "n_depth = 1\n",
    "n_inputs = n_height * n_width * n_depth  # total pixels\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(mnist.train.num_examples/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input images in shape n_samples,n_pixels\n",
    "x = tf.placeholder(dtype=tf.float32, name=\"x\", shape=[None, n_inputs]) \n",
    "# output labels\n",
    "y = tf.placeholder(dtype=tf.float32, name=\"y\", shape=[None, n_classes]) \n",
    "\n",
    "# reshape input to (n_samples,n_width,n_height,n_depth)\n",
    "x_ = tf.reshape(x, shape=[-1, n_width, n_height, n_depth])\n",
    "\n",
    "# create first set of convolutional layers\n",
    "layer1_w = tf.Variable(tf.random_normal(shape=[4,4,n_depth,32],\n",
    "                                        stddev=0.1),\n",
    "                       name='l1_w')\n",
    "layer1_b = tf.Variable(tf.random_normal([32]),\n",
    "                       name='l1_b')\n",
    "layer1_conv = tf.nn.relu(tf.nn.conv2d(x_,\n",
    "                                      layer1_w,\n",
    "                                      strides=[1,1,1,1],\n",
    "                                      padding='SAME'\n",
    "                                     ) + \n",
    "                         layer1_b \n",
    "                        )\n",
    "layer1_pool = tf.nn.max_pool(layer1_conv,\n",
    "                             ksize=[1,2,2,1],\n",
    "                             strides=[1,2,2,1],\n",
    "                             padding='SAME'\n",
    "                            )\n",
    "\n",
    "layer2_w = tf.Variable(tf.random_normal(shape=[4,4,32,64],\n",
    "                                        stddev=0.1),\n",
    "                       name='l2_w')\n",
    "layer2_b = tf.Variable(tf.random_normal([64]),\n",
    "                       name='l2_b')\n",
    "layer2_conv = tf.nn.relu(tf.nn.conv2d(layer1_pool,\n",
    "                                      layer2_w,\n",
    "                                      strides=[1,1,1,1],\n",
    "                                      padding='SAME'\n",
    "                                     ) + \n",
    "                         layer2_b \n",
    "                        )\n",
    "layer2_pool = tf.nn.max_pool(layer2_conv,\n",
    "                             ksize=[1,2,2,1],\n",
    "                             strides=[1,2,2,1],\n",
    "                             padding='SAME'\n",
    "                            )\n",
    "\n",
    "\n",
    "layer3_w = tf.Variable(tf.random_normal(shape=[64*7*7*1,1024],\n",
    "                                        stddev=0.1),\n",
    "                       name='l3_w')\n",
    "layer3_b = tf.Variable(tf.random_normal([1024]),\n",
    "                       name='l3_b')\n",
    "layer3_fc = tf.nn.relu(tf.matmul(tf.reshape(layer2_pool,\n",
    "                                            [-1, 64*7*7*1]),\n",
    "                                 layer3_w) +\n",
    "                       layer3_b\n",
    "                      )\n",
    "\n",
    "layer4_w = tf.Variable(tf.random_normal(shape=[1024, n_classes],\n",
    "                                        stddev=0.1),\n",
    "                       name='l4_w'\n",
    "                      )\n",
    "layer4_b = tf.Variable(tf.random_normal([n_classes]),name='l4_b')\n",
    "\n",
    "layer4_out = tf.matmul(layer3_fc,layer4_w)+layer4_b\n",
    "\n",
    "model = layer4_out\n",
    "\n",
    "# loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y)\n",
    "loss = tf.reduce_mean(entropy)\n",
    "# optimizer function\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000   loss = 2.142813\n",
      "Epoch: 0001   loss = 0.108121\n",
      "Epoch: 0002   loss = 0.077320\n",
      "Epoch: 0003   loss = 0.062054\n",
      "Epoch: 0004   loss = 0.050821\n",
      "Epoch: 0005   loss = 0.044723\n",
      "Epoch: 0006   loss = 0.034373\n",
      "Epoch: 0007   loss = 0.030643\n",
      "Epoch: 0008   loss = 0.029777\n",
      "Epoch: 0009   loss = 0.024144\n",
      "Model Trained.\n",
      "Accuracy: 0.9854\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate\n",
    "with tf.Session() as tfs:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch in range(n_batches):\n",
    "            batch_x,batch_y = mnist.train.next_batch(batch_size)\n",
    "            feed_dict={x:batch_x, y: batch_y}\n",
    "            batch_loss,_ = tfs.run([loss, optimizer],\n",
    "                                   feed_dict=feed_dict\n",
    "                                  )\n",
    "            total_loss += batch_loss \n",
    "        average_loss = total_loss / n_batches\n",
    "        print(\"Epoch: {0:04d}   loss = {1:0.6f}\".format(epoch,average_loss))\n",
    "    print(\"Model Trained.\")\n",
    "\n",
    "    predictions_check = tf.equal(tf.argmax(model,1),tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions_check, tf.float32))\n",
    "    feed_dict = {x:mnist.test.images, y:mnist.test.labels}\n",
    "    print(\"Accuracy:\", accuracy.eval(feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CNN with Keras for MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D, Dense, Flatten, Reshape\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        544       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              3212288   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 3,255,914\n",
      "Trainable params: 3,255,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 5s 86us/step - loss: 0.9424 - acc: 0.7545\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 5s 86us/step - loss: 0.2490 - acc: 0.9253\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 5s 84us/step - loss: 0.1760 - acc: 0.9473\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 5s 83us/step - loss: 0.1368 - acc: 0.9597\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 4s 81us/step - loss: 0.1125 - acc: 0.9661\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 5s 86us/step - loss: 0.0961 - acc: 0.9712\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 5s 85us/step - loss: 0.0842 - acc: 0.9752\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 4s 81us/step - loss: 0.0751 - acc: 0.9778\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 5s 82us/step - loss: 0.0686 - acc: 0.9800\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 5s 90us/step - loss: 0.0623 - acc: 0.9811\n",
      "10000/10000 [==============================] - 1s 57us/step\n",
      "\n",
      "Test loss: 0.0613170108855\n",
      "Test accuracy: 0.9796\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "keras.backend.clear_session()\n",
    "\n",
    "n_filters=[32,64]\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Reshape(target_shape=(n_width,n_height,n_depth), \n",
    "                  input_shape=(n_inputs,)\n",
    "                 )\n",
    "         )\n",
    "\n",
    "model.add(Conv2D(filters=n_filters[0], \n",
    "                 kernel_size=4, \n",
    "                 padding='SAME', \n",
    "                 activation='relu' \n",
    "                ) \n",
    "         )\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2), \n",
    "                       strides=(2,2) \n",
    "                      ) \n",
    "         )\n",
    "\n",
    "model.add(Conv2D(filters=n_filters[1], \n",
    "                 kernel_size=4, \n",
    "                 padding='SAME', \n",
    "                 activation='relu', \n",
    "                ) \n",
    "         )\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2), \n",
    "                       strides=(2,2) \n",
    "                      ) \n",
    "         )\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=1024, activation='relu'))\n",
    "model.add(Dense(units=n_classes, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=n_epochs)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test)\n",
    "print('\\nTest loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
