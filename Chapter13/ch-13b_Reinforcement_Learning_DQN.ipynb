{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#import-the-libraries\" data-toc-modified-id=\"import-the-libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>import the libraries</a></span></li><li><span><a href=\"#Functions-for-discretizing-the-observation-values\" data-toc-modified-id=\"Functions-for-discretizing-the-observation-values-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Functions for discretizing the observation values</a></span></li><li><span><a href=\"#initialize-the-polecart-environment\" data-toc-modified-id=\"initialize-the-polecart-environment-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>initialize the polecart environment</a></span></li><li><span><a href=\"#Q-Table-based-algorithm\" data-toc-modified-id=\"Q-Table-based-algorithm-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Q Table based algorithm</a></span></li><li><span><a href=\"#Q-Network-based-algorithm\" data-toc-modified-id=\"Q-Network-based-algorithm-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Q Network based algorithm</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:1.13.3\n",
      "TensorFlow:1.4.0\n",
      "Keras:2.0.9\n",
      "OpenAI Gym: 0.9.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "print(\"NumPy:{}\".format(np.__version__))\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(123)\n",
    "print(\"TensorFlow:{}\".format(tf.__version__))\n",
    "\n",
    "import keras\n",
    "print(\"Keras:{}\".format(keras.__version__))\n",
    "\n",
    "import gym\n",
    "print('OpenAI Gym:',gym.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for discretizing the observation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretize the value to a state space\n",
    "def discretize(val,bounds,n_states):\n",
    "    discrete_val = 0\n",
    "    if val <= bounds[0]:\n",
    "        discrete_val = 0\n",
    "    elif val >= bounds[1]:\n",
    "        discrete_val = n_states-1\n",
    "    else:\n",
    "        discrete_val = int(round( (n_states-1) * \n",
    "                                  ((val-bounds[0])/\n",
    "                                   (bounds[1]-bounds[0])) \n",
    "                                ))\n",
    "    return discrete_val\n",
    "\n",
    "def discretize_state(vals,s_bounds,n_s):\n",
    "    discrete_vals = []\n",
    "    for i in range(len(n_s)):\n",
    "        discrete_vals.append(discretize(vals[i],s_bounds[i],n_s[i]))\n",
    "    return np.array(discrete_vals,dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize the polecart environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "n_a = env.action_space.n\n",
    "# number of discrete states for each observation dimension\n",
    "n_s = np.array([10,10,10,10])   # position, velocity, angle, angular velocity\n",
    "s_bounds = np.array(list(zip(env.observation_space.low, env.observation_space.high)))\n",
    "# the velocity and angular velocity bounds are too high so we bound between -1, +1\n",
    "s_bounds[1] = (-1.0,1.0) \n",
    "s_bounds[3] = (-1.0,1.0)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Table based algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:policy_q_table, Min reward:8.0, Max reward:180.0, Average reward:17.592\n"
     ]
    }
   ],
   "source": [
    "def policy_q_table(state, env):\n",
    "    # Exploration strategy - Select a random action\n",
    "    if np.random.random() < explore_rate:\n",
    "        action = env.action_space.sample()\n",
    "    # Exploitation strategy - Select the action with the highest q\n",
    "    else:\n",
    "        action = np.argmax(q_table[tuple(state)])\n",
    "    return action\n",
    "\n",
    "def episode(env, policy, r_max=0, t_max=0):\n",
    "\n",
    "    # observe initial state\n",
    "    obs = env.reset()\n",
    "    state_prev = discretize_state(obs,s_bounds,n_s)\n",
    "    \n",
    "    # initialize the variables\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    t = 0\n",
    "    while not done:\n",
    "        \n",
    "        # select an action, and observe the next state\n",
    "        action = policy(state_prev, env)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        state_new = discretize_state(obs,s_bounds,n_s)\n",
    "\n",
    "        # Update the Q-table \n",
    "        best_q = np.amax(q_table[tuple(state_new)])\n",
    "        bellman_q = reward + discount_rate * best_q\n",
    "        indices = tuple(np.append(state_prev,action))\n",
    "        q_table[indices] += learning_rate*( bellman_q - q_table[indices])\n",
    "    \n",
    "        # set next state as current state\n",
    "        state_prev = state_new\n",
    "        \n",
    "        episode_reward += reward\n",
    "        if r_max > 0 and episode_reward > r_max:\n",
    "            break\n",
    "        t+=1\n",
    "        if t_max > 0 and t == t_max:\n",
    "            break\n",
    "    return episode_reward\n",
    "    \n",
    "    #if return_hist_reward>=episode_reward:\n",
    "    #    return_val = [np.array(o_list),np.array(a_list),np.array(r_list)]\n",
    "    #else:\n",
    "    #    return_val = episode_reward\n",
    "    #return return_val\n",
    "\n",
    "# collect observations and rewards for each episode\n",
    "def experiment(env, policy, n_episodes,r_max=0, t_max=0):\n",
    "    \n",
    "    rewards=np.empty(shape=[n_episodes])\n",
    "    for i in range(n_episodes):\n",
    "        val = episode(env, policy, r_max, t_max)\n",
    "        rewards[i]=val\n",
    "            \n",
    "    print('Policy:{}, Min reward:{}, Max reward:{}, Average reward:{}'\n",
    "      .format(policy.__name__,\n",
    "              np.min(rewards),\n",
    "              np.max(rewards),\n",
    "              np.mean(rewards)))\n",
    "\n",
    "\n",
    "# create a q-table of shape (10,10,10,10, 2) representing S X A -> R\n",
    "q_table = np.zeros(shape = np.append(n_s,n_a))    \n",
    "    \n",
    "learning_rate = 0.8\n",
    "discount_rate = 0.9\n",
    "explore_rate = 0.2\n",
    "n_episodes = 1000\n",
    "\n",
    "experiment(env, policy_q_table, n_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Network based algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 58\n",
      "Trainable params: 58\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Policy:policy_q_nn, Min reward:8.0, Max reward:150.0, Average reward:41.27\n"
     ]
    }
   ],
   "source": [
    "def policy_q_nn(obs, env):\n",
    "    # Exploration strategy - Select a random action\n",
    "    if np.random.random() < explore_rate:\n",
    "        action = env.action_space.sample()\n",
    "    # Exploitation strategy - Select the action with the highest q\n",
    "    else:\n",
    "        action = np.argmax(q_nn.predict(np.array([obs])))\n",
    "    return action\n",
    "\n",
    "def episode(env, policy, r_max=0, t_max=0):\n",
    "\n",
    "    # observe initial state\n",
    "    obs = env.reset()\n",
    "    state_prev = discretize_state(obs,s_bounds,n_s)\n",
    "    \n",
    "    # initialize the variables\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    t = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        action = policy(state_prev, env)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        state_next = discretize_state(obs,s_bounds,n_s)\n",
    "                                             \n",
    "        # add the state_prev, action, reward, state_new, done to memory\n",
    "        memory.append([state_prev,action,reward,state_next,done])\n",
    "                           \n",
    "        \n",
    "        # Generate and update the q_values with \n",
    "        # maximum future rewards using bellman function:\n",
    "        states = np.array([x[0] for x in memory])\n",
    "        states_next = np.array([np.zeros(4) if x[4] else x[3] for x in memory])\n",
    "        \n",
    "        q_values = q_nn.predict(states)\n",
    "        q_values_next = q_nn.predict(states_next)\n",
    "        \n",
    "        for i in range(len(memory)):\n",
    "            state_prev,action,reward,state_next,done = memory[i]\n",
    "            if done:\n",
    "                q_values[i,action] = reward\n",
    "            else:\n",
    "                best_q = np.amax(q_values_next[i])\n",
    "                bellman_q = reward + discount_rate * best_q\n",
    "                q_values[i,action] = bellman_q\n",
    "        \n",
    "        # train the q_nn with states and q_values, same as updating the q_table\n",
    "        q_nn.fit(states,q_values,epochs=1,batch_size=50,verbose=0)\n",
    "    \n",
    "        state_prev = state_next\n",
    "        \n",
    "        episode_reward += reward\n",
    "        if r_max > 0 and episode_reward > r_max:\n",
    "            break\n",
    "        t+=1\n",
    "        if t_max > 0 and t == t_max:\n",
    "            break\n",
    "    return episode_reward\n",
    "\n",
    "# experiment collect observations and rewards for each episode\n",
    "def experiment(env, policy, n_episodes,r_max=0, t_max=0):\n",
    "    \n",
    "    rewards=np.empty(shape=[n_episodes])\n",
    "    for i in range(n_episodes):\n",
    "        \n",
    "        val = episode(env, policy, r_max, t_max)\n",
    "        #print('episode:{}, reward {}'.format(i,val))\n",
    "        rewards[i]=val\n",
    "            \n",
    "    print('Policy:{}, Min reward:{}, Max reward:{}, Average reward:{}'\n",
    "        .format(policy.__name__,\n",
    "              np.min(rewards),\n",
    "              np.max(rewards),\n",
    "              np.mean(rewards)))\n",
    "\n",
    "# create the empty list to contain game memory\n",
    "from collections import deque \n",
    "memory = deque(maxlen=1000)\n",
    "\n",
    "# build the Q-Network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(8,input_dim=4, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "model.summary()\n",
    "q_nn = model\n",
    "    \n",
    "learning_rate = 0.8\n",
    "discount_rate = 0.9\n",
    "explore_rate = 0.2\n",
    "n_episodes = 100\n",
    "\n",
    "experiment(env, policy_q_nn, n_episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
